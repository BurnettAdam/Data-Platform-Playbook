(window.webpackJsonp=window.webpackJsonp||[]).push([[12],{112:function(e,t,a){"use strict";a.d(t,"a",(function(){return u})),a.d(t,"b",(function(){return m}));var n=a(0),r=a.n(n);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var c=r.a.createContext({}),p=function(e){var t=r.a.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},u=function(e){var t=p(e.components);return r.a.createElement(c.Provider,{value:t},e.children)},b={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},d=r.a.forwardRef((function(e,t){var a=e.components,n=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(a),d=n,m=u["".concat(l,".").concat(d)]||u[d]||b[d]||o;return a?r.a.createElement(m,i(i({ref:t},c),{},{components:a})):r.a.createElement(m,i({ref:t},c))}));function m(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=a.length,l=new Array(o);l[0]=d;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i.mdxType="string"==typeof e?e:n,l[1]=i;for(var c=2;c<o;c++)l[c]=a[c];return r.a.createElement.apply(null,l)}return r.a.createElement.apply(null,a)}d.displayName="MDXCreateElement"},80:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return l})),a.d(t,"metadata",(function(){return i})),a.d(t,"toc",(function(){return s})),a.d(t,"default",(function(){return p}));var n=a(3),r=a(7),o=(a(0),a(112)),l={title:"Ingest manually uploaded csv files",description:"Ingest data from csv files",layout:"playbook_js",tags:"playbook"},i={unversionedId:"playbook/ingest-data-from-csv-files",id:"playbook/ingest-data-from-csv-files",isDocsHomePage:!1,title:"Ingest manually uploaded csv files",description:"Ingest data from csv files",source:"@site/docs/playbook/ingest-data-from-csv-files.md",slug:"/playbook/ingest-data-from-csv-files",permalink:"/Data-Platform-Playbook/playbook/ingest-data-from-csv-files",editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/playbook/ingest-data-from-csv-files.md",version:"current",sidebar:"docs",previous:{title:"Exporting database snapshot to the Data Platform Landing Zone",permalink:"/Data-Platform-Playbook/playbook/exporting-snapshot-to-landing-zone"},next:{title:"Exporting database snapshots to the Data Platform Landing Zone",permalink:"/Data-Platform-Playbook/docs/exporting-snapshot-to-landing-zone"}},s=[{value:"Prerequisites",id:"prerequisites",children:[]},{value:"Steps",id:"steps",children:[]}],c={toc:s};function p(e){var t=e.components,a=Object(r.a)(e,["components"]);return Object(o.b)("wrapper",Object(n.a)({},c,a,{components:t,mdxType:"MDXLayout"}),Object(o.b)("h2",{id:"prerequisites"},"Prerequisites"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"You have some structured data files in CSV format you wish to have available from the Data Platform"),Object(o.b)("li",{parentName:"ul"},"You have access to the Hackney Data Platform"),Object(o.b)("li",{parentName:"ul"},"The department you are placing this data into the data platform has the manual CSV upload\nfunctionality enabled.")),Object(o.b)("h2",{id:"steps"},"Steps"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"Sign in to the AWS Management Console and open the ",Object(o.b)("a",{parentName:"p",href:"https://console.aws.amazon.com/s3/"},"Amazon S3 console"),".")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"In the Buckets list, choose a landing zone bucket, either the ",Object(o.b)("inlineCode",{parentName:"p"},"dataplatform-stg-landing-zone"),"\nor ",Object(o.b)("inlineCode",{parentName:"p"},"dataplatform-prod-landing-zone")," bucket.\nNavigate to your departments manual upload folder, see below for structure."),Object(o.b)("pre",{parentName:"li"},Object(o.b)("code",{parentName:"pre"},"<department>/\n\u2514\u2500\u2500 manual/\n"))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},'Inside of the manual folder, create a new folder for the dataset you wish to create inside the data platform.\nThe name of the folder you create here will be used throughout the platform, e.g. if you create a folder\ncalled "cake-designs" you will see a table called "cake_designs" within ',Object(o.b)("a",{parentName:"p",href:"/Data-Platform-Playbook/playbook/querying-data-using-sql"},"AWS Athena"),".\nIf you are appending data to an already existing dataset, you can skip this step.")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"Inside of your dataset folder, upload a CSV containing your dataset.  Any CSVs uploaded within this folder\nwill be combined into one dataset, and should have a matching set of columns.")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"Now switch to ",Object(o.b)("a",{parentName:"p",href:"https://eu-west-2.console.aws.amazon.com/glue/home?region=eu-west-2#etl:tab=jobs"},"AWS Glue Jobs"),", where you will run the job called\n",Object(o.b)("inlineCode",{parentName:"p"},"<department> copy manually uploaded CSVs to raw"),'.\nSelect this job and click the "Run job" option underneath the Action menu.')),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},'Observe the progress of this job using the "History" tab, and wait for the "Run status" to reach "Succeeded".\nThis job will have created a S3 folder structure as shown below inside the Raw zone, and an Apache Parquet file\ncontaining your CSV data inside of there.'),Object(o.b)("pre",{parentName:"li"},Object(o.b)("code",{parentName:"pre"},"<department>/\n\u2514\u2500\u2500 manual/\n    \u2514\u2500\u2500 <dataset-name>/\n        \u2514\u2500\u2500 import_year=<year>/\n            \u2514\u2500\u2500 import_month=<month>/\n                \u2514\u2500\u2500 import_day=<day>/\n"))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"To access this data within ",Object(o.b)("a",{parentName:"p",href:"/Data-Platform-Playbook/playbook/querying-data-using-sql"},"AWS Athena"),", you will need to crawl this data, using\nthe matching crawler.  Navigate to the ",Object(o.b)("a",{parentName:"p",href:"https://eu-west-2.console.aws.amazon.com/glue/home?region=eu-west-2#catalog:tab=crawlerss"},"AWS Glue Crawler")," interface, find the job\nnamed ",Object(o.b)("inlineCode",{parentName:"p"},"raw-zone-<department>-manual-uploads-crawler"),', and select "Run crawler".  Observe the job within the\nconsole until it\'s status returns to "Ready".')),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"Once crawled, there will be a newly created table within the database ",Object(o.b)("inlineCode",{parentName:"p"},"raw-zone-<department>-manual-uploads-database"),".\nYou can then view the newly imported tables under the tables tab."))))}p.isMDXComponent=!0}}]);