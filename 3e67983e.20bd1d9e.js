(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{122:function(e,t,a){"use strict";a.d(t,"a",(function(){return p})),a.d(t,"b",(function(){return y}));var r=a(0),n=a.n(r);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var c=n.a.createContext({}),u=function(e){var t=n.a.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},p=function(e){var t=u(e.components);return n.a.createElement(c.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.a.createElement(n.a.Fragment,{},t)}},m=n.a.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,i=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),p=u(a),m=r,y=p["".concat(i,".").concat(m)]||p[m]||d[m]||o;return a?n.a.createElement(y,s(s({ref:t},c),{},{components:a})):n.a.createElement(y,s({ref:t},c))}));function y(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=a[c];return n.a.createElement.apply(null,i)}return n.a.createElement.apply(null,a)}m.displayName="MDXCreateElement"},82:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return i})),a.d(t,"metadata",(function(){return s})),a.d(t,"toc",(function(){return l})),a.d(t,"default",(function(){return u}));var r=a(3),n=a(7),o=(a(0),a(122)),i={title:"Guide to testing data quality in Glue Jobs",description:"A guide to continuous data quality testing in Glue Jobs",layout:"playbook_js",tags:"playbook"},s={unversionedId:"playbook/data-quality-testing-guide",id:"playbook/data-quality-testing-guide",isDocsHomePage:!1,title:"Guide to testing data quality in Glue Jobs",description:"A guide to continuous data quality testing in Glue Jobs",source:"@site/docs/playbook/data-quality-testing-guide.md",slug:"/playbook/data-quality-testing-guide",permalink:"/Data-Platform-Playbook/playbook/data-quality-testing-guide",editUrl:"https://github.com/LBHackney-IT/data-platform-playbook/edit/master/docs/playbook/data-quality-testing-guide.md",version:"current",sidebar:"docs",previous:{title:"Connecting to the redshift cluster from Google Data Studio",permalink:"/Data-Platform-Playbook/playbook/connecting-to-redshift-from-data-studio"},next:{title:"Exporting database snapshot to the Data Platform Landing Zone",permalink:"/Data-Platform-Playbook/playbook/exporting-snapshot-to-landing-zone"}},l=[{value:"Resources",id:"resources",children:[]},{value:"Prerequisites",id:"prerequisites",children:[{value:"Example Check",id:"example-check",children:[]},{value:"Example Anomaly Detection",id:"example-anomaly-detection",children:[]}]}],c={toc:l};function u(e){var t=e.components,a=Object(n.a)(e,["components"]);return Object(o.b)("wrapper",Object(r.a)({},c,a,{components:t,mdxType:"MDXLayout"}),Object(o.b)("h2",{id:"resources"},"Resources"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"https://github.com/awslabs/python-deequ"},"PyDeequ README"))),Object(o.b)("h2",{id:"prerequisites"},"Prerequisites"),Object(o.b)("p",null,"Update the job arguments of your Glue job to include:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Extra jars: ",Object(o.b)("inlineCode",{parentName:"li"},"--extra-jars = s3://dataplatform-stg-glue-scripts/jars/deequ-1.0.3.jar")," "),Object(o.b)("li",{parentName:"ul"},"Extra Python files: ",Object(o.b)("inlineCode",{parentName:"li"},"--extra-py-file = s3://dataplatform-stg-glue-scripts/python-modules/pydeequ-1.0.1.zip")," "),Object(o.b)("li",{parentName:"ul"},"Metrics repository S3 target location using the template format:\n",Object(o.b)("inlineCode",{parentName:"li"},"--deequ_metrics_location = s3://dataplatform-stg-EXAMPLE-zone/quality-metrics/department=EXAMPLE/dataset=EXAMPLE/deequ-metrics.json"))),Object(o.b)("h3",{id:"example-check"},"Example Check"),Object(o.b)("p",null,"Here is an example of using deequ checks to validate a dataframe, and storing related metrics to S3.\nThe ",Object(o.b)("inlineCode",{parentName:"p"},"description_of_work")," column is checked to be complete, and ",Object(o.b)("inlineCode",{parentName:"p"},"work_priority_priority_code")," between\n1 and 4 inclusively."),Object(o.b)("pre",null,Object(o.b)("code",{parentName:"pre",className:"language-python"},'from helpers import get_metrics_target_location\nfrom pydeequ.checks import Check, CheckLevel\nfrom pydeequ.repository import FileSystemMetricsRepository, ResultKey\nfrom pydeequ.verification import VerificationSuite, VerificationResult, RelativeRateOfChangeStrategy\n\nmetrics_target_location = get_metrics_target_location()\n\nmetricsRepository = FileSystemMetricsRepository(spark_session, metrics_target_location)\nresultKey = ResultKey(spark_session, ResultKey.current_milli_time(), {})\n\ncheckResult = VerificationSuite(spark_session) \\\n    .onData(df) \\\n    .useRepository(metricsRepository) \\\n    .addCheck(Check(spark_session, CheckLevel.Error, "data quality checks") \\\n        .hasMin("work_priority_priority_code", lambda x: x >= 1) \\\n        .hasMax("work_priority_priority_code", lambda x: x <= 4)  \\\n        .isComplete("description_of_work")) \\\n    .saveOrAppendResult(resultKey) \\\n    .run()\n    \ncheckResult_df = VerificationResult.checkResultsAsDataFrame(spark_session, checkResult)\ncheckResult_df.show()\n')),Object(o.b)("p",null,"Here is a ",Object(o.b)("a",{parentName:"p",href:"https://pydeequ.readthedocs.io/en/latest/pydeequ.html#module-pydeequ.checks"},"list of checks")," that are available to use."),Object(o.b)("h3",{id:"example-anomaly-detection"},"Example Anomaly Detection"),Object(o.b)("p",null,"Anomaly detection uses historic metrics to determine if a value is invalid.\nFor example, we check if the size of a dataframe has increased by more than twice the previous size."),Object(o.b)("pre",null,Object(o.b)("code",{parentName:"pre",className:"language-python"},"from helpers import get_metrics_target_location\nfrom pydeequ.verification import VerificationSuite, VerificationResult, RelativeRateOfChangeStrategy\nfrom pydeequ.repository import FileSystemMetricsRepository, ResultKey\n\nmetrics_target_location = get_metrics_target_location()\n\nmetricsRepository = FileSystemMetricsRepository(spark_session, metrics_target_location)\nresultKey = ResultKey(spark_session, ResultKey.current_milli_time(), {})\n\nanomalyCheckResult = VerificationSuite(spark_session) \\\n    .onData(df) \\\n    .useRepository(metricsRepository) \\\n    .addAnomalyCheck(RelativeRateOfChangeStrategy(maxRateIncrease = 2.0), Size()) \\\n    .saveOrAppendResult(resultKey) \\\n    .run()\n\nanomalyCheckResult_df = VerificationResult.checkResultsAsDataFrame(spark_session, anomalyCheckResult)\nanomalyCheckResult_df.show()\n")),Object(o.b)("p",null,"Here is a ",Object(o.b)("a",{parentName:"p",href:"https://pydeequ.readthedocs.io/en/latest/pydeequ.html#module-pydeequ.checks"},"list of anomaly detection types")," that are available to use."))}u.isMDXComponent=!0}}]);